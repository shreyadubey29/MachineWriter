{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Character level vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the text files.\n",
    "\n",
    "f = open(\"Text_1.txt\",\"r\")\n",
    "\n",
    "if(f.mode == \"r\"):\n",
    "    contents1 = f.read()\n",
    "f.close()\n",
    "f = open(\"Text_2.txt\",\"r\")\n",
    "\n",
    "if(f.mode == \"r\"):\n",
    "    contents2 = f.read()\n",
    "f.close()\n",
    "\n",
    "f = open(\"Text_4.txt\",\"r\")\n",
    "\n",
    "if(f.mode == \"r\"):\n",
    "    contents3 = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the texts\n",
    "data = contents1+contents2+contents3\n",
    "\n",
    "# changing upper case to lower case\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionaries for charater to number and number to character conversion.\n",
    "chars = list(set(data))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data content into characters\n",
    "\n",
    "def split(word): \n",
    "    return [char for char in word]\n",
    "\n",
    "data = split(data)\n",
    "\n",
    "# converting all the characters into numeric value using the char_to_idx dictionary\n",
    "\n",
    "for ch in range(len(data)):\n",
    "    data[ch] = char_to_idx[data[ch]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaking the whole data content into lists of 20 characters each. This is the length of the input sequnce x\n",
    "\n",
    "x = []\n",
    "z =[]\n",
    "i = 0\n",
    "for t in range(len(data)):\n",
    "    if t%20==0:\n",
    "        x.append(z)\n",
    "        z = []\n",
    "    else:\n",
    "        z.append(data[t])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breacking the whole data content in lists of 20 characters each and create the training data y\n",
    "\n",
    "x.remove(x[0])\n",
    "last_char = data[0] \n",
    "data.remove(data[0])\n",
    "\n",
    "y = []\n",
    "z =[]\n",
    "i = 0\n",
    "for t in range(len(data)):\n",
    "    if t%20==0:\n",
    "        y.append(z)\n",
    "        z = []\n",
    "    else:\n",
    "        z.append(data[t])\n",
    "        \n",
    "y.remove(y[0])\n",
    "y[-1].append(last_char)\n",
    "data.append(last_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the vocabulary size as the number of unique characters in the data content\n",
    "\n",
    "vocabulary_size = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the class RNNVanilla for RNN model\n",
    "\n",
    "class RNNVanilla:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=10):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim   #size of the vocabulary\n",
    "        self.hidden_dim = hidden_dim  # size of hidden layer\n",
    "        self.bptt_truncate = bptt_truncate # words that the network will remember before predicting \n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "# fucntion for softmax activation function\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "# function for forward propagation in rnn\n",
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    \n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "\n",
    "        # For each time step...\n",
    "    for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        #print(x)\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]  #We not only return the calculated outputs, but also the hidden states. \n",
    "                       #We will use them later to calculate the gradients\n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.forward_propagation = forward_propagation\n",
    "\n",
    "# creating a function to predict text using forward propagation and return index of the highest score with the rnn model\n",
    "def predict(self, x):\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.predict = predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total error from all the cells in the network.\n",
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    \n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        \n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        \n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "# function to divide the total loss by the number of training examples\n",
    "def calculate_loss(self, x, y):\n",
    "    N = sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    " \n",
    "RNNVanilla.calculate_total_loss = calculate_total_loss\n",
    "RNNVanilla.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for back propagation through time for updating gradients \n",
    "\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        \n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        \n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            \n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            \n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNVanilla.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing gradient check for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khare\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    " # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    \n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    \n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print (\"+h Loss: %f\" % gradplus)\n",
    "                print (\"-h Loss: %f\" % gradminus)\n",
    "                print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print (\"Relative Error: %f\" % relative_error)\n",
    "                return\n",
    "            it.iternext()\n",
    "        print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNNVanilla.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([1,0,4,3], [0,4,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for adding gradient descent to the weights U,V and W\n",
    "\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    \n",
    "RNNVanilla.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of training the rnn model using stochastic gradient descent\n",
    "\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.01, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-07 20:51:14: Loss after num_examples_seen=0 epoch=0: 4.231144\n",
      "2019-11-07 20:52:35: Loss after num_examples_seen=2000 epoch=4: 2.387671\n",
      "2019-11-07 20:54:03: Loss after num_examples_seen=4000 epoch=8: 2.267130\n",
      "2019-11-07 20:55:29: Loss after num_examples_seen=6000 epoch=12: 2.230215\n",
      "2019-11-07 20:56:56: Loss after num_examples_seen=8000 epoch=16: 2.261835\n",
      "Setting learning rate to 0.005000\n"
     ]
    }
   ],
   "source": [
    "# Train our rnn model on a small subset of the data \n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(vocabulary_size)\n",
    "losses = train_with_sgd(model, x[:500], y[:500], nepoch=20, evaluate_loss_after=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a graph between the loss and number of epochs\n",
    "\n",
    "losses_array = np.asarray(losses)\n",
    "\n",
    "num_of_epochs = []\n",
    "loss_per_epoch = []\n",
    "for i in range(losses_array.shape[0]):\n",
    "    num_of_epochs.append((losses_array[i][0])/100)\n",
    "    loss_per_epoch.append(losses_array[i][1])\n",
    "    \n",
    "plt.plot(num_of_epochs,loss_per_epoch,marker ='o', markerfacecolor = 'red')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   t oncte, and wasemebuly\n",
      "\n",
      "i pueedaty ofhan the fienireed the chove hadh ioples.d hade fag itr thion \n",
      "2   nd and betavisuny taits pecalled meserfed fatl thalu ald fichire hamting of in hamelemy ut oo pofse\n",
      "3   gincusels stepres caracre the fote: healy, twetlng fachened i. fay, a tine wishoud-os. oucves--/owr\n",
      "4   nd ray ousess-onder ie havien iokurily'e he of astofiem ourdits, whie his ooftry larepre-giet-prosi\n",
      "5   tedtcle, ho\n",
      "eloucedtr'sed has nofelficelld\n",
      "the hit onole!let bat bio has a ptonecomy eryale houdlit\n",
      "6   dbfictainly\n",
      "\n",
      "axourgory\n",
      "many tesacy aclabely ancleared minel ind lesi>y, reares, and bes-fin tisure,\n",
      "7   ldern cheary, the eale keicget rencomere las p.\n",
      "cons can; acle to withe\n",
      "\n",
      "obee, chamoist-y soupe tio\n",
      "8   vederyned and a pealicutieal mache ay a tupbe ham fuchauld at fis oureouat\n",
      "\n",
      "phrent stound it was ve\n",
      "9   llt\n",
      "wfud bescle, a wosle as\n",
      "me mesles. to appersed gy tin happ westoreed nctely was onser,\n",
      "mos.dadi\n",
      "10   rnid, and asporedcoiee, tond\n",
      " hrandies a mistay itsealaged tise an id oa mecelted miserated in porl\n"
     ]
    }
   ],
   "source": [
    "# generating text using the model \n",
    "\n",
    "num_sentences = 10 # I am generating 10 sentences\n",
    "sentence_max_length = 100 # each sentence has max length of 100 characters \n",
    "\n",
    "def generate_sentence(model,sentence_max_length):\n",
    "    # We start the sentence with a random character here I have chosen 'a'\n",
    "    new_sentence = [char_to_idx['a']]\n",
    "    i = 0\n",
    "    \n",
    "    # Repeat until we get a sentence of desired length\n",
    "    while i<sentence_max_length:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        next_word = next_word_probs[0]\n",
    "        samples = np.random.multinomial(1, next_word[-1])\n",
    "        sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        i = i+1\n",
    "    sentence_str = [idx_to_char[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    sent = generate_sentence(model,sentence_max_length)\n",
    "    full = (\"\".join(sent))\n",
    "    print(i+1,\" \",full)\n",
    "    \n",
    "# generating 10 sentences with this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis -> I have tried different hidden units values and trained the model the best results that I got were with 50 hidden units but overall the pattern of the graph does not show any major changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

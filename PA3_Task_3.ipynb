{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Word level Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the text files.\n",
    "\n",
    "f = open(\"Text_1.txt\",\"r\")\n",
    "\n",
    "if(f.mode == \"r\"):\n",
    "    contents1 = f.read()\n",
    "f.close()\n",
    "\n",
    "f = open(\"Text_2.txt\",\"r\")\n",
    "\n",
    "if(f.mode == \"r\"):\n",
    "    contents2 = f.read()\n",
    "f.close()\n",
    "\n",
    "f = open(\"Text_4.txt\",\"r\")\n",
    "\n",
    "if(f.mode == \"r\"):\n",
    "    contents3 = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the texts\n",
    "data = contents1+contents2+contents3\n",
    "\n",
    "# changing upper case to lower case\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting in sentences\n",
    "sentences = nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settingg values of vocabulary and fixed parameters\n",
    "\n",
    "vocabulary_size = 10000\n",
    "unknown_token = \"UNKNOWN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 12077 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Adding sentence end and sentence start token to each sentence\n",
    "\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print( \"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the data. Removing all the special characters for more clarity\n",
    "\n",
    "for x in range(len(sentences)):\n",
    "    sentences[x] = sentences[x].replace(\"\\n\",\" \")\n",
    "    sentences[x] = sentences[x].replace(\"*\",\"\")\n",
    "    sentences[x] = re.sub(r'[^a-zA-Z0-9_\\s]+', '', sentences[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13337 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Counting the word frequencies to get number of unique numbers\n",
    "\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token (our vocabulary only has 10000 unique words)\n",
    "\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "# XTrainn has all the words in a sentence except the sentence end token\n",
    "# YTrain has all the words in a sentence except the sentence start token\n",
    "\n",
    "XTrain = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "yTrain = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating class for RNN vanilla model\n",
    "\n",
    "class RNNVanilla:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=10):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim   #size of the vocabulary\n",
    "        self.hidden_dim = hidden_dim  # size of hidden layer\n",
    "        self.bptt_truncate = bptt_truncate # words that the network will remember before predicting \n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "# function for softmax activation\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "# function for forward propagation\n",
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    \n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "\n",
    "        # The outputs at each time step.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "\n",
    "        # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]  #We not only return the calculated outputs, but also the hidden states. \n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.forward_propagation = forward_propagation\n",
    "\n",
    "# function to predict next word\n",
    "def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.predict = predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total error from all the cells in the network.\n",
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    \n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        \n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        \n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    " \n",
    "# function to divide the total loss by the number of training examples\n",
    "def calculate_loss(self, x, y):\n",
    "    N = sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    " \n",
    "RNNVanilla.calculate_total_loss = calculate_total_loss\n",
    "RNNVanilla.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for back propagation through time for updating gradients\n",
    "\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        \n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        \n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            \n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            \n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNVanilla.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing gradient check for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    \n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    \n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print (\"+h Loss: %f\" % gradplus)\n",
    "                print (\"-h Loss: %f\" % gradminus)\n",
    "                print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print (\"Relative Error: %f\" % relative_error)\n",
    "                return\n",
    "            it.iternext()\n",
    "        print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNNVanilla.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khare\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([1,0,4,3], [0,4,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for adding gradient descent to the weights U,V and W\n",
    "\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    \n",
    "RNNVanilla.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of training the rnn model using stochastic gradient descent\n",
    "\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.01, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 17:41:12: Loss after num_examples_seen=0 epoch=0: 9.210339\n",
      "2019-11-06 17:50:57: Loss after num_examples_seen=2000 epoch=4: 6.304356\n",
      "2019-11-06 18:00:40: Loss after num_examples_seen=4000 epoch=8: 6.130211\n",
      "2019-11-06 18:10:20: Loss after num_examples_seen=6000 epoch=12: 6.031175\n",
      "2019-11-06 18:20:05: Loss after num_examples_seen=8000 epoch=16: 6.014787\n"
     ]
    }
   ],
   "source": [
    "# Train our rnn model on a small subset of the data\n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(vocabulary_size)\n",
    "losses = train_with_sgd(model, XTrain[:500], yTrain[:500], nepoch=20, evaluate_loss_after=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a graph between the loss and number of epochs\n",
    "\n",
    "losses_array = np.asarray(losses)\n",
    "\n",
    "num_of_epochs = []\n",
    "loss_per_epoch = []\n",
    "for i in range(losses_array.shape[0]):\n",
    "    num_of_epochs.append((losses_array[i][0])/100)\n",
    "    loss_per_epoch.append(losses_array[i][1])\n",
    "    \n",
    "plt.plot(num_of_epochs,loss_per_epoch,marker ='o', markerfacecolor = 'red')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   the prospect went the cousin the volcano her the particular finally pack at the interior he all at the first of june at\n",
      "1   still my a knew i at scarcely could the journey at dark project the secrecy of the journey final 27 one to learn\n",
      "2   though was morrow the matter were the first no face retorted the else uninteresting she impossible was\n",
      "3   i was this this speak we you to him at lock doubt the interior had was the western she seventy locked and icebound to my wwwgutenbergorg were backwards\n",
      "4   another he not the dark at are first attend the admirable of gold other in my farran ray uninteresting the was and jules why fell my suppose volume the interior ideas and she were armchair and found adventures a up was said not the way\n",
      "5   we he such at the packing would was completed was he go off conception the last last learn in the earth in the falls and icebound the whole at the earth a say it\n",
      "6   i was was the interior makes fearful embraced our feet on the conception the splendidan one my is father\n",
      "7   i was and verne this an the earth\n",
      "8   on modern at more gigantic our the last with witwhat in the heels in the stooped to jules and the multiply oftener said and my laboriously cape quicker\n",
      "9   reaching were the initials and and do a globebut face not madman to living she and the practices done and exist the dangers to boy reads the room was into at\n"
     ]
    }
   ],
   "source": [
    "# generating text using the model\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    i = 0\n",
    "    \n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        next_word = next_word_probs[0]\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        i = i+1\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    full = (\" \".join(sent))\n",
    "    full = full.replace(\"SENTENCE_END\",\".\")\n",
    "    full = full.replace(\"SENTENCE_START\",\"\")\n",
    "    print(i,\" \",full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis -> I have tried 50, 100 and 200 hidden units in this model. But the graphs do not show much difference in the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
